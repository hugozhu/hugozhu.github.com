<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Concurrent Programming on All about Raspberry Pi</title>
    <link>https://blog.hugozhu.site/tags/concurrent-programming/</link>
    <description>Recent content in Concurrent Programming on All about Raspberry Pi</description>
    <generator>Hugo</generator>
    <language>en</language>
    <managingEditor>hugozhu@gmail.com (Hugo Zhu)</managingEditor>
    <webMaster>hugozhu@gmail.com (Hugo Zhu)</webMaster>
    <lastBuildDate>Sat, 20 Apr 2013 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://blog.hugozhu.site/tags/concurrent-programming/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Go语言内存模型</title>
      <link>https://blog.hugozhu.site/post/2013/31-golang-memory-model/</link>
      <pubDate>Sat, 20 Apr 2013 00:00:00 +0000</pubDate><author>hugozhu@gmail.com (Hugo Zhu)</author>
      <guid>https://blog.hugozhu.site/post/2013/31-golang-memory-model/</guid>
      <description>名词定义 执行体 - Go里的Goroutine或Java中的Thread&#xA;背景介绍 内存模型的目的是为了定义清楚变量的读写在不同执行体里的可见性。理解内存模型在并发编程中非常重要，因为代码的执行顺序和书写的逻辑顺序并不会完全一致，甚至在编译期间编译器也有可能重排代码以最优化CPU执行, 另外还因为有CPU缓存的存在，内存的数据不一定会及时更新，这样对内存中的同一个变量读和写也不一定和期望一样。&#xA;和Java的内存模型规范类似，Go语言也有一个内存模型，相对JMM来说，Go的内存模型比较简单，Go的并发模型是基于CSP（Communicating Sequential Process）的，不同的Goroutine通过一种叫Channel的数据结构来通信；Java的并发模型则基于多线程和共享内存，有较多的概念（violatie, lock, final, construct, thread, atomic等）和场景，当然java.util.concurrent并发工具包大大简化了Java并发编程。&#xA;Go内存模型规范了在什么条件下一个Goroutine对某个变量的修改一定对其它Goroutine可见。&#xA;Happens Before 在一个单独的Goroutine里，对变量的读写和代码的书写顺序一致。比如以下的代码:&#xA;package main import ( &amp;#34;log&amp;#34; ) var a, b, c int func main() { a = 1 b = 2 c = a + 2 log.Println(a, b, c) } 尽管在编译期和执行期，编译器和CPU都有可能重排代码，比如，先执行b=2，再执行a=1，但c=a+2是保证在a=1后执行的。这样最后的执行结果一定是1 2 3，不会是1 2 2。但下面的代码则可能会输出0 0 0，1 2 2, 0 2 3 (b=2比a=1先执行), 1 2 3等各种可能。&#xA;package main import ( &amp;#34;log&amp;#34; ) var a, b, c int func main() { go func() { a = 1 b = 2 }() go func() { c = a + 2 }() log.</description>
    </item>
    <item>
      <title>并发编程之内存屏障</title>
      <link>https://blog.hugozhu.site/post/2013/22-memory-barriers-or-fences/</link>
      <pubDate>Thu, 28 Mar 2013 00:00:00 +0000</pubDate><author>hugozhu@gmail.com (Hugo Zhu)</author>
      <guid>https://blog.hugozhu.site/post/2013/22-memory-barriers-or-fences/</guid>
      <description>原文地址：http://mechanical-sympathy.blogspot.com/2011/07/memory-barriersfences.html 或 http://ifeve.com/memory-barriersfences/&#xA;关键词：Load Barrier, Store Barrier, Full Barrier&#xA;本文我将和大家讨论并发编程中最基础的一项技术：内存屏障或内存栅栏，也就是让一个CPU处理单元中的内存状态对其它处理单元可见的一项技术。&#xA;CPU使用了很多优化技术来达成一个事实：CPU执行单元的速度要远超主存访问速度。在我上一篇文章 &amp;ldquo;Write Combing - 合并写&amp;quot;中我已经介绍了其中的一项技术。CPU避免内存访问延迟最常见的技术是将指令管道化，然后尽量重排这些管道的执行以最大利用缓存而把因为缓存未命中引起的延迟降到最小。&#xA;当一个程序执行时指令是否被重排并不重要，只要最终的结果是一样的。例如，在一个循环里，如果循环体内没用到这个计数器，循环的计数器什么时候更新（在循环开始，中间还是最后）并不重要。编译器和CPU可以自由的重排指令以最佳的利用CPU，只要下一次循环前更新该计数器即可。并且在循环执行中，这个变量可能一直存在寄存器上，并没有被推到缓存或主存，这样这个变量对其他CPU来说一直都是不可见的。&#xA;CPU核内部包含了多个执行单元。例如，现代Intel CPU包含了6个执行单元，可以组合进行算术运算，逻辑条件判断及内存操作。每个执行单元可以执行上述任务的某种组合。这些执行单元是并行执行的，这样指令也就是在并行执行。但如果站在另一个CPU角度看，这也就产生了程序顺序的另一种不确定性。&#xA;最后，当一个缓存失效发生时，现代CPU可以先假设一个内存载入的值并根据这个假设值继续执行，直到内存载入返回确切的值。&#xA;CPU核 | V 寄存器 | V 执行单元 -&amp;gt; Load/Store缓冲区-&amp;gt;L1 Cache ---&amp;gt;L3 Cache--&amp;gt;内存控制器--&amp;gt;主存 | | +-&amp;gt; Write Combine缓冲区-&amp;gt;L2 Cache ---+ 代码顺序并不是真正的执行顺序，CPU和编译器可以各种优化只要有空间提高性能。缓存和主存的读取会利用load, store和write-combining缓冲区来缓冲和重排。这些缓冲区是查找速度很快的关联队列，当一个后来发生的load需要读取上一个store的值，而该值还没有到达缓存，查找是必需的，上图描绘的是一个简化的现代多核CPU，从上图可以看出执行单元可以利用本地寄存器和缓冲区来管理和缓存子系统的交互。&#xA;在多线程环境里需要使用技术来使得程序结果尽快可见。这篇文章里我不会涉及到 Cache Conherence 的概念。请先假定一个事实：一旦内存数据被推送到缓存，就会有消息协议来确保所有的缓存会对所有的共享数据同步并保持一致。这个使内存数据对CPU核可见的技术被称为内存屏障或内存栅栏。&#xA;内存屏障提供了两个功能。首先，它们通过确保从另一个CPU来看屏障的两边的所有指令都是正确的程序顺序，而保持程序顺序的外部可见性；其次它们可以实现内存数据可见性，确保内存数据会同步到CPU缓存子系统。&#xA;大多数的内存屏障都是复杂的话题。在不同的CPU架构上内存屏障的实现非常不一样。相对来说Intel CPU的强内存模型比DEC Alpha的弱复杂内存模型（缓存不仅分层了，还分区了）更简单。因为x86处理器是在多线程编程中最常见的，下面我尽量用x86的架构来阐述。&#xA;Store Barrier Store屏障，是x86的&amp;rdquo;sfence&amp;ldquo;指令，强制所有在store屏障指令之前的store指令，都在该store屏障指令执行之前被执行，并把store缓冲区的数据都刷到CPU缓存。这会使得程序状态对其它CPU可见，这样其它CPU可以根据需要介入。一个实际的好例子是Disruptor中的BatchEventProcessor。当序列Sequence被一个消费者更新时，其它消费者(Consumers)和生产者（Producers）知道该消费者的进度，因此可以采取合适的动作。所以屏障之前发生的内存更新都可见了。&#xA;private volatile long sequence = RingBuffer.INITIAL_CURSOR_VALUE; // from inside the run() method T event = null; long nextSequence = sequence.</description>
    </item>
    <item>
      <title>Java并发包中的同步队列SynchronousQueue实现原理</title>
      <link>https://blog.hugozhu.site/post/2013/java-synchronousqueue-notes/</link>
      <pubDate>Tue, 05 Mar 2013 00:00:00 +0000</pubDate><author>hugozhu@gmail.com (Hugo Zhu)</author>
      <guid>https://blog.hugozhu.site/post/2013/java-synchronousqueue-notes/</guid>
      <description>介绍 Java 6的并发编程包中的SynchronousQueue是一个没有数据缓冲的BlockingQueue，生产者线程对其的插入操作put必须等待消费者的移除操作take，反过来也一样。&#xA;不像ArrayBlockingQueue或LinkedListBlockingQueue，SynchronousQueue内部并没有数据缓存空间，你不能调用peek()方法来看队列中是否有数据元素，因为数据元素只有当你试着取走的时候才可能存在，不取走而只想偷窥一下是不行的，当然遍历这个队列的操作也是不允许的。队列头元素是第一个排队要插入数据的线程，而不是要交换的数据。数据是在配对的生产者和消费者线程之间直接传递的，并不会将数据缓冲数据到队列中。可以这样来理解：生产者和消费者互相等待对方，握手，然后一起离开。&#xA;SynchronousQueue的一个使用场景是在线程池里。Executors.newCachedThreadPool()就使用了SynchronousQueue，这个线程池根据需要（新任务到来时）创建新的线程，如果有空闲线程则会重复使用，线程空闲了60秒后会被回收。&#xA;实现原理 同步队列的实现方法有许多：&#xA;阻塞算法实现 阻塞算法实现通常在内部采用一个锁来保证多个线程中的put()和take()方法是串行执行的。采用锁的开销是比较大的，还会存在一种情况是线程A持有线程B需要的锁，B必须一直等待A释放锁，即使A可能一段时间内因为B的优先级比较高而得不到时间片运行。所以在高性能的应用中我们常常希望规避锁的使用。&#xA;public class NativeSynchronousQueue&amp;lt;E&amp;gt; { boolean putting = false; E item = null; public synchronized E take() throws InterruptedException { while (item == null) wait(); E e = item; item = null; notifyAll(); return e; } public synchronized void put(E e) throws InterruptedException { if (e==null) return; while (putting) wait(); putting = true; item = e; notifyAll(); while (item!=null) wait(); putting = false; notifyAll(); } } 信号量实现 经典同步队列实现采用了三个信号量，代码很简单，比较容易理解：</description>
    </item>
  </channel>
</rss>
